{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1f566d-075c-426b-b55a-886b4a661838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import mdmm\n",
    "from torch import optim\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import zuko\n",
    "from zuko.flows import TransformModule, SimpleAffineTransform\n",
    "from zuko.distributions import BoxUniform\n",
    "from zuko.distributions import DiagNormal\n",
    "from memflow.unfolding_flow.utils import Compute_ParticlesTensor as particle_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038f2902-d6cf-4055-9a43-1e645f2c48c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from memflow.read_data import dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b120429d-d731-4223-b1e2-bbfebc8e92c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#root =\"/work/adpetre/datasets/dataset_part1\"\n",
    "root_lxplus = '/eos/user/a/adpetre/www/ttHbbAnalysis/training_dataset/v3_sig_forTrainingDataset/all_jets_fullRun2_ttHbb_forTraining_allyears_spanetprov_part1_validation.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da177ab-6536-427d-a251-ee5fe26d96d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Loading datasets\n",
      "Loading partons in LAB\n",
      "PartonLevel LAB\n",
      "Reading parton_level Files\n",
      "Load logScaled_data_higgs_t_tbar_ISR\n",
      "Parton: Move tensors to device (cuda) memory\n",
      "Loading reco in LAB\n",
      "RecoLevel LAB\n",
      "Reading reco_level Files\n",
      "Load recoParticles\n",
      "Load scaledLogRecoParticles\n",
      "Reco: Move tensors to device (cuda) memory\n",
      "Loaded datasets:  ['partons_lab', 'reco_lab']\n"
     ]
    }
   ],
   "source": [
    "data = dataset_all.DatasetCombined(root_lxplus, dtype=torch.float64, dev=device, datasets=[\"partons_lab\", \"reco_lab\"],\n",
    "                                    reco_list_lab=['recoParticles',\n",
    "                                                'scaledLogRecoParticles', 'mask_lepton',\n",
    "                                                'mask_jets','mask_met',\n",
    "                                                'mask_boost', 'data_boost'],\n",
    "                                    parton_list_lab=['logScaled_data_higgs_t_tbar_ISR',\n",
    "                                                 'flattening_weight_HEta_tHadEta_tLepEta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac1121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 18, 8])\n",
      "torch.Size([1000, 18])\n",
      "torch.Size([1000, 4, 3])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "No_ev = 1000\n",
    "\n",
    "scaledLogReco = data.reco_lab.scaledLogRecoParticles[0:No_ev]\n",
    "scaledLogParton = data.partons_lab.logScaled_data_higgs_t_tbar_ISR[0:No_ev]\n",
    "maskJets = data.reco_lab.mask_jets[0:No_ev]\n",
    "maskLepton = data.reco_lab.mask_lepton[0:No_ev]\n",
    "maskMET = data.reco_lab.mask_met[0:No_ev]\n",
    "\n",
    "boostReco = data.reco_lab.data_boost[0:No_ev]\n",
    "maskBoost = data.reco_lab.mask_boost[0:No_ev]\n",
    "\n",
    "maskedReco = torch.cat((maskJets, maskLepton, maskMET), dim=1)\n",
    "\n",
    "scaledJets = scaledLogReco[:,:16]\n",
    "leptonMET = scaledLogReco[:,16:]\n",
    "\n",
    "print(scaledLogReco.shape)\n",
    "print(maskedReco.shape)\n",
    "print(scaledLogParton.shape)\n",
    "print(scaledLogReco.is_cuda)\n",
    "\n",
    "# ordering is not needed cuz jets are already ordered by pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 0,  1,  2,  3,  7,  8,  9, 10, 11, 12, 13, 14, 15,  4,  5,  6])\n",
      "tensor([ 2.0397,  0.9433,  0.2977,  0.2543,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5110, -1.3750, -1.3766])\n",
      "torch.Size([10000, 16, 8])\n",
      "torch.Size([10000, 16])\n",
      "torch.Size([10000, 16, 1])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2.370302438735962 1.9843720197677612 1.88614821434021 -0.176679790019989\n",
      " -0.3284093141555786 -0.36130616068840027 -0.3630555272102356\n",
      " -1.2532942295074463 -1.4020713567733765 -1.454121708869934 -- -- -- -- --\n",
      " --]\n"
     ]
    }
   ],
   "source": [
    "indices = torch.argsort(scaledJets[:,:,0], dim=1, descending=True)\n",
    "print(maskJets[0])\n",
    "print(indices[0])\n",
    "print(scaledJets[0, indices[0], 0])\n",
    "\n",
    "print(scaledJets.shape)\n",
    "\n",
    "maskJets_newDim = maskJets[..., np.newaxis]\n",
    "print(maskJets.shape)\n",
    "print(maskJets_newDim.shape)\n",
    "\n",
    "print(maskJets[0])\n",
    "\n",
    "masked_b = np.ma.masked_array(*np.broadcast_arrays(scaledJets, np.logical_not(maskJets_newDim)))\n",
    "\n",
    "print(masked_b[4,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b613b",
   "metadata": {},
   "source": [
    "# transformer encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d937b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "input_features = [8, 3]\n",
    "transformer_model = nn.Transformer(d_model=64,\n",
    "                                   nhead=4,\n",
    "                                   num_encoder_layers=3,\n",
    "                                   num_decoder_layers=3,\n",
    "                                   dim_feedforward=64,\n",
    "                                   activation=nn.GELU(),\n",
    "                                   batch_first=True)\n",
    "\n",
    "linearDNN_reco = nn.Linear(in_features=8, out_features=64)\n",
    "linearDNN_parton = nn.Linear(in_features=3, out_features=64)\n",
    "linearDNN_boost = nn.Linear(in_features=4, out_features=64)\n",
    "gelu = nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c10dbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledLogReco_afterLin = gelu(linearDNN_reco(scaledLogReco) * maskedReco[..., None])\n",
    "scaledLogParton_afterLin = gelu(linearDNN_parton(scaledLogParton))\n",
    "boost_afterLin = gelu(linearDNN_boost(boostReco) * maskBoost[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d15806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 18, 64])\n",
      "torch.Size([10000, 4, 64])\n",
      "torch.Size([10000, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(scaledLogReco_afterLin.shape)\n",
    "print(scaledLogParton_afterLin.shape)\n",
    "print(boost_afterLin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e14c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledLogReco_withBoost_afterLin = torch.cat((scaledLogReco_afterLin, boost_afterLin), dim=1)\n",
    "output_decoder1 = transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f774b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.forward() got an unexpected keyword argument 'tgt_is_causal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                    nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                    num_encoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mGELU(),\n\u001b[1;32m      7\u001b[0m                                    batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m transformer_model\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(scaledLogReco_afterLin\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m output_decoder2 \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaledLogParton_afterLin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaledLogReco_afterLin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m output_decoder3 \u001b[38;5;241m=\u001b[39m transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n",
      "File \u001b[0;32m/work/adpetre/miniconda3/envs/dizertatie/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.forward() got an unexpected keyword argument 'tgt_is_causal'"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(d_model=64,\n",
    "                                   nhead=4,\n",
    "                                   num_encoder_layers=3,\n",
    "                                   num_decoder_layers=3,\n",
    "                                   dim_feedforward=128,\n",
    "                                   activation=nn.GELU(),\n",
    "                                   batch_first=True)\n",
    "\n",
    "tgt_mask = transformer_model.generate_square_subsequent_mask(scaledLogReco_afterLin.size(1))\n",
    "#output_decoder2 = transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin, tgt_is_causal=True)\n",
    "output_decoder3 = transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin, tgt_mask=tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "055ce8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f817378",
   "metadata": {},
   "source": [
    "# Initialize flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05c42d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 18, 64])\n",
      "torch.Size([10000, 18, 8])\n"
     ]
    }
   ],
   "source": [
    "print(output_decoder.shape)\n",
    "print(scaledLogReco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d125a20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m flow \u001b[38;5;241m=\u001b[39m zuko\u001b[38;5;241m.\u001b[39mflows\u001b[38;5;241m.\u001b[39mNSF(features\u001b[38;5;241m=\u001b[39mflow_nfeatures,\n\u001b[1;32m      7\u001b[0m                       context\u001b[38;5;241m=\u001b[39mflow_ncond, \n\u001b[1;32m      8\u001b[0m                       transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                       univariate_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbound\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m }, \u001b[38;5;66;03m# Keeping the flow in the [-B,B] box.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                       passes\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flow_autoregressive \u001b[38;5;28;01melse\u001b[39;00m flow_nfeatures)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_objects):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m flow(\u001b[43moutput_decoder\u001b[49m[:,\u001b[38;5;28mobject\u001b[39m])\u001b[38;5;241m.\u001b[39mlog_prob(scaledLogReco[:,\u001b[38;5;28mobject\u001b[39m]) \u001b[38;5;66;03m# i will want to add the boost in the target too\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_decoder' is not defined"
     ]
    }
   ],
   "source": [
    "flow_nfeatures = 8\n",
    "flow_ncond = 64\n",
    "no_objects = 18\n",
    "flow_autoregressive = True\n",
    "\n",
    "flow = zuko.flows.NSF(features=flow_nfeatures,\n",
    "                      context=flow_ncond, \n",
    "                      transforms=2, \n",
    "                      bins=10, \n",
    "                      hidden_features=[3]*64, \n",
    "                      randperm=False,\n",
    "                      base=DiagNormal,\n",
    "                      base_args=[torch.ones(flow_nfeatures)*0, torch.ones(flow_nfeatures)*0.25],\n",
    "                      univariate_kwargs={\"bound\": 1 }, # Keeping the flow in the [-B,B] box.\n",
    "                      passes= 2 if not flow_autoregressive else flow_nfeatures)\n",
    "\n",
    "for object in range(no_objects):\n",
    "    x = flow(output_decoder[:,object]).log_prob(scaledLogReco[:,object]) # i will want to add the boost in the target too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f04dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferFlow(nn.Module):\n",
    "    def __init__(self,\n",
    "                 no_recoVars, no_partonVars,\n",
    "                 out_features_linearDNN=64,\n",
    "                 \n",
    "                 transformer_nhead=8,\n",
    "                 transformer_num_encoder_layers=4,\n",
    "                 transformer_num_decoder_layers=4,\n",
    "                 transformer_dim_feedforward=128,\n",
    "                 transformer_activation=nn.GELU(),\n",
    "                 \n",
    "                 flow_nfeatures=12,\n",
    "                 flow_ncond=34, \n",
    "                 flow_ntransforms=5,\n",
    "                 flow_hiddenMLP_NoLayers=4,\n",
    "                 flow_hiddenMLP_LayerDim=128,\n",
    "                 flow_bins=16,\n",
    "                 flow_autoregressive=True,\n",
    "                 flow_base=BoxUniform,\n",
    "                 flow_base_first_arg=-1,\n",
    "                 flow_base_second_arg=1,\n",
    "                 flow_bound=1.,\n",
    "                 randPerm=False,\n",
    "                 \n",
    "                 device=torch.device('cpu'),\n",
    "                 dtype=torch.float32,\n",
    "                 eps=1e-4):\n",
    "\n",
    "        super(TransferFlow, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.eps = eps # used for small values like the mass of the gluon for numerical reasons\n",
    "        \n",
    "        self.linearDNN_reco = nn.Linear(in_features=no_recoVars, out_features=out_features_linearDNN)\n",
    "        self.linearDNN_parton = nn.Linear(in_features=no_partonVars, out_features=out_features_linearDNN)\n",
    "        self.linearDNN_boost = nn.Linear(in_features=4, out_features=out_features_linearDNN)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.transformer_model = nn.Transformer(d_model=out_features_linearDNN,\n",
    "                                           nhead=transformer_nhead,\n",
    "                                           num_encoder_layers=transformer_num_encoder_layers,\n",
    "                                           num_decoder_layers=transformer_num_decoder_layers,\n",
    "                                           dim_feedforward=transformer_dim_feedforward,\n",
    "                                           activation=transformer_activation,\n",
    "                                           batch_first=True)\n",
    "         \n",
    "        \n",
    "        self.flow = zuko.flows.NSF(features=flow_nfeatures,\n",
    "                              context=out_features_linearDNN,\n",
    "                              transforms=flow_ntransforms, \n",
    "                              bins=flow_bins, \n",
    "                              hidden_features=[flow_hiddenMLP_LayerDim]*flow_hiddenMLP_NoLayers, \n",
    "                              randperm=randPerm,\n",
    "                              base=eval(flow_base),\n",
    "                              base_args=[torch.ones(flow_nfeatures)*flow_base_first_arg, torch.ones(flow_nfeatures)*flow_base_second_arg],\n",
    "                                   univariate_kwargs={\"bound\": flow_bound }, # Keeping the flow in the [-B,B] box.\n",
    "                              passes= 2 if not flow_autoregressive else flow_nfeatures)\n",
    "        \n",
    "\n",
    "    def disable_conditioner_regression_training(self):\n",
    "        ''' Disable the conditioner regression training, but keep the\n",
    "        latent space training'''\n",
    "        self.cond_transformer.disable_regression_training()\n",
    "\n",
    "    def enable_regression_training(self):\n",
    "        self.cond_transformer.enable_regression_training()\n",
    "        \n",
    "    def forward(self,  scaling_reco_lab, scaling_reco_lab_onlyPtEtaPhi, scaling_partons_lab, scaling_RegressedBoost_lab,\n",
    "                mask_reco, mask_boost, no_objects, flow_eval=\"normalizing\", Nsamples=0):\n",
    "        \n",
    "        scaledLogReco_afterLin = self.gelu(self.linearDNN_reco(scaling_reco_lab) * mask_reco[..., None])\n",
    "        scaledLogParton_afterLin = self.gelu(self.linearDNN_parton(scaling_partons_lab))\n",
    "        #boost_afterLin = self.gelu(self.linearDNN_boost(scaling_RegressedBoost_lab))\n",
    "        #scaledLogReco_withBoost_afterLin = torch.cat((scaledLogReco_afterLin, boost_afterLin), dim=1)\n",
    "        \n",
    "        \n",
    "        tgt_mask = self.transformer_model.generate_square_subsequent_mask(scaledLogReco_afterLin.size(1))\n",
    "        output_decoder = self.transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin,\n",
    "                                          tgt_mask=tgt_mask)\n",
    "        \n",
    "        flow_prob = self.flow(output_decoder[:,:no_objects]).log_prob(scaling_reco_lab_onlyPtEtaPhi[:,:no_objects])\n",
    "        \n",
    "        #tgt_mask = self.transformer_model.generate_square_subsequent_mask(scaledLogReco_withBoost_afterLin.size(1))\n",
    "        #output_decoder = self.transformer_model(scaledLogParton_afterLin, scaledLogReco_withBoost_afterLin,\n",
    "        #                                  tgt_mask=tgt_mask)\n",
    "        \n",
    "        #scaling_reco_lab_andBoost = torch.cat((scaling_reco_lab[:,:,:3], scaling_RegressedBoost_lab[:,:,1:]), dim=1)\n",
    "        #mask_reco_andBoost = torch.cat((mask_reco, mask_boost), dim=1)\n",
    "        #flow_prob = self.flow(output_decoder[:,:no_objects]).log_prob(scaling_reco_lab_andBoost[:,:no_objects])\n",
    "        # i will want to add the boost in the target too\n",
    "        \n",
    "        flow_prob_batch = torch.sum(flow_prob*mask_reco[:,:no_objects], dim=1) # take avg of masked objects\n",
    "        avg_flow_prob = flow_prob_batch.mean() # is this good?? \n",
    "                \n",
    "        return avg_flow_prob, flow_prob_batch, flow_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af62f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_flow = TransferFlow(no_recoVars=7, no_partonVars=3,\n",
    "                 out_features_linearDNN=64,\n",
    "                 \n",
    "                 transformer_nhead=4,\n",
    "                 transformer_num_encoder_layers=4,\n",
    "                 transformer_num_decoder_layers=4,\n",
    "                 transformer_dim_feedforward=64,\n",
    "                 transformer_activation=nn.GELU(),\n",
    "                 \n",
    "                 flow_nfeatures=3,\n",
    "                 flow_ncond=34, \n",
    "                 flow_ntransforms=5,\n",
    "                 flow_hiddenMLP_NoLayers=16,\n",
    "                 flow_hiddenMLP_LayerDim=128,\n",
    "                 flow_bins=16,\n",
    "                 flow_autoregressive=True,\n",
    "                 flow_base='DiagNormal',\n",
    "                 flow_base_first_arg=0,\n",
    "                 flow_base_second_arg=0.3,\n",
    "                 flow_bound=1.,\n",
    "                 randPerm=False,\n",
    "                 \n",
    "                 device=torch.device('cpu'),\n",
    "                 dtype=torch.float32,\n",
    "                 eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adff259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/adpetre/zuko/zuko/transforms.py:413: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:33.)\n",
      "  return torch.searchsorted(seq, value[..., None]).squeeze(dim=-1)\n"
     ]
    }
   ],
   "source": [
    "avg_flowPr, batch_flow_pr, flow_pr = tr_flow(scaledLogReco[:,:,:7], scaledLogReco[:,:,:3], scaledLogParton, boostReco, maskedReco, maskBoost,\n",
    "                                             10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9e768",
   "metadata": {},
   "source": [
    "# check loss per object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee58cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "first_try = torch.sum(flow_pr*maskedReco[:,:10], dim=0)\n",
    "print(first_try.shape)\n",
    "\n",
    "number_MaskedObjects = torch.sum(maskedReco[:,:10], dim=0)\n",
    "print(number_MaskedObjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3039f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10, 8])\n",
      "torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(scaledLogReco[:,:10].shape)\n",
    "print(flow_pr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b119f",
   "metadata": {},
   "source": [
    "# check loss in pt bins + unscale pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54d21a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mean_reco = data.reco_lab.meanRecoParticles\n",
    "log_std_reco = data.reco_lab.stdRecoParticles\n",
    "reco_particles = data.reco_lab.recoParticles[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77845d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_pt = torch.exp(scaledLogReco[:,:,0]*log_std_reco[0] + log_mean_reco[0]) - 1\n",
    "unscaled_pt = unscaled_pt*maskedReco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f288d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApsklEQVR4nO3de3RV5Z3/8U9CyEm4nASwySEaEKsN16pIjVFEXWQRbepIZU0VoyJSEA0dERcIM4rUUbHBS70gSl0Ka43KZVZV5GbTBGGKMUAEgQSjtlgo9iQzYs4BhADJ9/dHf9nDgQgBkjnJk/drrb3seZ7v3vv5HsHz6c7eOTFmZgIAAHBMbLQXAAAA0BIIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJ8VFewHRVF9fr6+//lpdu3ZVTExMtJcDAACawMy0b98+paWlKTb2+6/XtOuQ8/XXXys9PT3aywAAAGdg9+7dOu+88753vl2HnK5du0r6x5vk9/ujvBoAANAU4XBY6enp3uf492nXIafhR1R+v5+QAwBAG3OqW0248RgAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnHTaIWfdunW68cYblZaWppiYGL377rsR82ammTNnqmfPnkpMTFR2dra++OKLiJq9e/cqLy9Pfr9fycnJGjdunPbv3x9Rs3XrVl199dVKSEhQenq6CgoKTljL0qVL1bdvXyUkJGjQoEFauXLl6bYDAAAcddoh58CBA7r44os1d+7cRucLCgr0wgsv6JVXXlFpaak6d+6snJwcHTp0yKvJy8tTeXm5CgsLtXz5cq1bt04TJkzw5sPhsEaMGKHevXurrKxMc+bM0axZszR//nyv5qOPPtLo0aM1btw4bd68WSNHjtTIkSO1ffv2020JAAC4yM6CJHvnnXe81/X19RYIBGzOnDneWE1Njfl8Pnv77bfNzKyiosIk2caNG72aVatWWUxMjO3Zs8fMzF5++WXr1q2b1dbWejUPPfSQZWRkeK9/8YtfWG5ubsR6MjMz7Z577mny+kOhkEmyUCjU5H0AAEB0NfXzu1nvydm5c6eCwaCys7O9saSkJGVmZqqkpESSVFJSouTkZA0ZMsSryc7OVmxsrEpLS72aYcOGKT4+3qvJyclRZWWlvv32W6/m2PM01DScpzG1tbUKh8MRGwAAcFOzhpxgMChJSk1NjRhPTU315oLBoFJSUiLm4+Li1L1794iaxo5x7Dm+r6ZhvjGzZ89WUlKSt6Wnp59ui0C7t6Nvv2gvAQCapF09XTVjxgyFQiFv2717d7SXBAAAWkizhpxAICBJqqqqihivqqry5gKBgKqrqyPmjx49qr1790bUNHaMY8/xfTUN843x+Xzy+/0RGwAAcFOzhpw+ffooEAioqKjIGwuHwyotLVVWVpYkKSsrSzU1NSorK/NqiouLVV9fr8zMTK9m3bp1OnLkiFdTWFiojIwMdevWzas59jwNNQ3nAQAA7dtph5z9+/dry5Yt2rJli6R/3Gy8ZcsW7dq1SzExMZo8ebIef/xxLVu2TNu2bdOdd96ptLQ0jRw5UpLUr18/XX/99Ro/frw2bNig9evXa9KkSbr11luVlpYmSbrtttsUHx+vcePGqby8XIsXL9bzzz+vKVOmeOu4//77tXr1aj3zzDP67LPPNGvWLG3atEmTJk06+3cFAAC0faf72NaaNWtM0gnbmDFjzOwfj5E/8sgjlpqaaj6fz4YPH26VlZURx/jmm29s9OjR1qVLF/P7/TZ27Fjbt29fRM2nn35qQ4cONZ/PZ+eee6499dRTJ6xlyZIl9qMf/cji4+NtwIABtmLFitPqhUfIgdPzx6ILrCKjb7SXAaCda+rnd4yZWRQzVlSFw2ElJSUpFApxfw7QBEXFP1TaffHq99mOaC8FQDvW1M/vdvV0FQAAaD8IOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwUrOHnLq6Oj3yyCPq06ePEhMT9cMf/lD//u//LjPzasxMM2fOVM+ePZWYmKjs7Gx98cUXEcfZu3ev8vLy5Pf7lZycrHHjxmn//v0RNVu3btXVV1+thIQEpaenq6CgoLnbAQAAbVSzh5zf/OY3mjdvnl566SXt2LFDv/nNb1RQUKAXX3zRqykoKNALL7ygV155RaWlpercubNycnJ06NAhryYvL0/l5eUqLCzU8uXLtW7dOk2YMMGbD4fDGjFihHr37q2ysjLNmTNHs2bN0vz585u7JQAA0BZZM8vNzbW77747Yuzmm2+2vLw8MzOrr6+3QCBgc+bM8eZramrM5/PZ22+/bWZmFRUVJsk2btzo1axatcpiYmJsz549Zmb28ssvW7du3ay2ttareeihhywjI6PJaw2FQibJQqHQ6TcKtEN/LLrAKjL6RnsZANq5pn5+N/uVnCuvvFJFRUX6/PPPJUmffvqp/vSnP+mGG26QJO3cuVPBYFDZ2dnePklJScrMzFRJSYkkqaSkRMnJyRoyZIhXk52drdjYWJWWlno1w4YNU3x8vFeTk5OjyspKffvtt42urba2VuFwOGIDAABuimvuA06fPl3hcFh9+/ZVhw4dVFdXpyeeeEJ5eXmSpGAwKElKTU2N2C81NdWbCwaDSklJiVxoXJy6d+8eUdOnT58TjtEw161btxPWNnv2bP36179uhi4BAEBr1+xXcpYsWaI333xTb731lj755BMtXLhQTz/9tBYuXNjcpzptM2bMUCgU8rbdu3dHe0kAAKCFNPuVnKlTp2r69Om69dZbJUmDBg3SX//6V82ePVtjxoxRIBCQJFVVValnz57eflVVVbrkkkskSYFAQNXV1RHHPXr0qPbu3evtHwgEVFVVFVHT8Lqh5ng+n08+n+/smwQAAK1es1/J+e677xQbG3nYDh06qL6+XpLUp08fBQIBFRUVefPhcFilpaXKysqSJGVlZammpkZlZWVeTXFxserr65WZmenVrFu3TkeOHPFqCgsLlZGR0eiPqgAAQPvS7CHnxhtv1BNPPKEVK1boq6++0jvvvKNnn31WP//5zyVJMTExmjx5sh5//HEtW7ZM27Zt05133qm0tDSNHDlSktSvXz9df/31Gj9+vDZs2KD169dr0qRJuvXWW5WWliZJuu222xQfH69x48apvLxcixcv1vPPP68pU6Y0d0sAAKANavYfV7344ot65JFHdN9996m6ulppaWm65557NHPmTK9m2rRpOnDggCZMmKCamhoNHTpUq1evVkJCglfz5ptvatKkSRo+fLhiY2M1atQovfDCC958UlKS/vCHPyg/P1+XXXaZzjnnHM2cOTPid+kAAID2K8bsmF9F3M6Ew2ElJSUpFArJ7/dHezlAq1dU/EOl3Revfp/tiPZSALRjTf385rurAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOCkFgk5e/bs0e23364ePXooMTFRgwYN0qZNm7x5M9PMmTPVs2dPJSYmKjs7W1988UXEMfbu3au8vDz5/X4lJydr3Lhx2r9/f0TN1q1bdfXVVyshIUHp6ekqKChoiXYAAEAb1Owh59tvv9VVV12ljh07atWqVaqoqNAzzzyjbt26eTUFBQV64YUX9Morr6i0tFSdO3dWTk6ODh065NXk5eWpvLxchYWFWr58udatW6cJEyZ48+FwWCNGjFDv3r1VVlamOXPmaNasWZo/f35ztwQAANoia2YPPfSQDR069Hvn6+vrLRAI2Jw5c7yxmpoa8/l89vbbb5uZWUVFhUmyjRs3ejWrVq2ymJgY27Nnj5mZvfzyy9atWzerra2NOHdGRkaT1xoKhUyShUKhJu8DtGd/LLrAKjL6RnsZANq5pn5+N/uVnGXLlmnIkCH653/+Z6WkpOjSSy/V7373O29+586dCgaDys7O9saSkpKUmZmpkpISSVJJSYmSk5M1ZMgQryY7O1uxsbEqLS31aoYNG6b4+HivJicnR5WVlfr2228bXVttba3C4XDEBgAA3NTsIecvf/mL5s2bp4suukgffPCB7r33Xv3Lv/yLFi5cKEkKBoOSpNTU1Ij9UlNTvblgMKiUlJSI+bi4OHXv3j2iprFjHHuO482ePVtJSUnelp6efpbdAgCA1qrZQ059fb0GDx6sJ598UpdeeqkmTJig8ePH65VXXmnuU522GTNmKBQKedvu3bujvSQAANBCmj3k9OzZU/37948Y69evn3bt2iVJCgQCkqSqqqqImqqqKm8uEAiouro6Yv7o0aPau3dvRE1jxzj2HMfz+Xzy+/0RGwAAcFOzh5yrrrpKlZWVEWOff/65evfuLUnq06ePAoGAioqKvPlwOKzS0lJlZWVJkrKyslRTU6OysjKvpri4WPX19crMzPRq1q1bpyNHjng1hYWFysjIiHiSCwAAtE/NHnIeeOABffzxx3ryySf15Zdf6q233tL8+fOVn58vSYqJidHkyZP1+OOPa9myZdq2bZvuvPNOpaWlaeTIkZL+ceXn+uuv1/jx47VhwwatX79ekyZN0q233qq0tDRJ0m233ab4+HiNGzdO5eXlWrx4sZ5//nlNmTKluVsCAABtUUs82vX+++/bwIEDzefzWd++fW3+/PkR8/X19fbII49Yamqq+Xw+Gz58uFVWVkbUfPPNNzZ69Gjr0qWL+f1+Gzt2rO3bty+i5tNPP7WhQ4eaz+ezc88915566qnTWiePkAOnh0fIAbQGTf38jjEzi3bQipZwOKykpCSFQiHuzwGaoKj4h0q7L179PtsR7aUAaMea+vnNd1cBAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEktHnKeeuopxcTEaPLkyd7YoUOHlJ+frx49eqhLly4aNWqUqqqqIvbbtWuXcnNz1alTJ6WkpGjq1Kk6evRoRM2HH36owYMHy+fz6cILL9SCBQtauh0AANBGtGjI2bhxo1599VX9+Mc/jhh/4IEH9P7772vp0qVau3atvv76a918883efF1dnXJzc3X48GF99NFHWrhwoRYsWKCZM2d6NTt37lRubq6uu+46bdmyRZMnT9Yvf/lLffDBBy3ZEgAAaCushezbt88uuugiKywstGuuucbuv/9+MzOrqamxjh072tKlS73aHTt2mCQrKSkxM7OVK1dabGysBYNBr2bevHnm9/uttrbWzMymTZtmAwYMiDjnLbfcYjk5OU1eYygUMkkWCoXOtE2gXflj0QVWkdE32ssA0M419fO7xa7k5OfnKzc3V9nZ2RHjZWVlOnLkSMR437591atXL5WUlEiSSkpKNGjQIKWmpno1OTk5CofDKi8v92qOP3ZOTo53jMbU1tYqHA5HbAAAwE1xLXHQRYsW6ZNPPtHGjRtPmAsGg4qPj1dycnLEeGpqqoLBoFdzbMBpmG+YO1lNOBzWwYMHlZiYeMK5Z8+erV//+tdn3BcAAGg7mv1Kzu7du3X//ffrzTffVEJCQnMf/qzMmDFDoVDI23bv3h3tJQEAgBbS7CGnrKxM1dXVGjx4sOLi4hQXF6e1a9fqhRdeUFxcnFJTU3X48GHV1NRE7FdVVaVAICBJCgQCJzxt1fD6VDV+v7/RqziS5PP55Pf7IzYAAOCmZg85w4cP17Zt27RlyxZvGzJkiPLy8rz/3bFjRxUVFXn7VFZWateuXcrKypIkZWVladu2baqurvZqCgsL5ff71b9/f6/m2GM01DQcAwAAtG/Nfk9O165dNXDgwIixzp07q0ePHt74uHHjNGXKFHXv3l1+v1+/+tWvlJWVpSuuuEKSNGLECPXv31933HGHCgoKFAwG9fDDDys/P18+n0+SNHHiRL300kuaNm2a7r77bhUXF2vJkiVasWJFc7cEAADaoBa58fhUnnvuOcXGxmrUqFGqra1VTk6OXn75ZW++Q4cOWr58ue69915lZWWpc+fOGjNmjB577DGvpk+fPlqxYoUeeOABPf/88zrvvPP02muvKScnJxotAQCAVibGzCzai4iWcDispKQkhUIh7s8BmqCo+IdKuy9e/T7bEe2lAGjHmvr5zXdXAQAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAGiSHX37RXsJAHBaCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQAwAAnETIAQAATiLkAAAAJxFyAACAkwg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDgAAcFKzh5zZs2frJz/5ibp27aqUlBSNHDlSlZWVETWHDh1Sfn6+evTooS5dumjUqFGqqqqKqNm1a5dyc3PVqVMnpaSkaOrUqTp69GhEzYcffqjBgwfL5/Ppwgsv1IIFC5q7HQAA0EY1e8hZu3at8vPz9fHHH6uwsFBHjhzRiBEjdODAAa/mgQce0Pvvv6+lS5dq7dq1+vrrr3XzzTd783V1dcrNzdXhw4f10UcfaeHChVqwYIFmzpzp1ezcuVO5ubm67rrrtGXLFk2ePFm//OUv9cEHHzR3SwAAoC2yFlZdXW2SbO3atWZmVlNTYx07drSlS5d6NTt27DBJVlJSYmZmK1eutNjYWAsGg17NvHnzzO/3W21trZmZTZs2zQYMGBBxrltuucVycnKavLZQKGSSLBQKnXF/QHtRkdHX/lh0gVVk9PXGnv5FbhRXBKC9aurnd4vfkxMKhSRJ3bt3lySVlZXpyJEjys7O9mr69u2rXr16qaSkRJJUUlKiQYMGKTU11avJyclROBxWeXm5V3PsMRpqGo7RmNraWoXD4YgNAAC4qUVDTn19vSZPnqyrrrpKAwcOlCQFg0HFx8crOTk5ojY1NVXBYNCrOTbgNMw3zJ2sJhwO6+DBg42uZ/bs2UpKSvK29PT0s+4RAAC0Ti0acvLz87V9+3YtWrSoJU/TZDNmzFAoFPK23bt3R3tJAACghcS11IEnTZqk5cuXa926dTrvvPO88UAgoMOHD6umpibiak5VVZUCgYBXs2HDhojjNTx9dWzN8U9kVVVVye/3KzExsdE1+Xw++Xy+s+4NAAC0fs1+JcfMNGnSJL3zzjsqLi5Wnz59IuYvu+wydezYUUVFRd5YZWWldu3apaysLElSVlaWtm3bpurqaq+msLBQfr9f/fv392qOPUZDTcMxAABA+9bsV3Ly8/P11ltv6b333lPXrl29e2iSkpKUmJiopKQkjRs3TlOmTFH37t3l9/v1q1/9SllZWbriiiskSSNGjFD//v11xx13qKCgQMFgUA8//LDy8/O9KzETJ07USy+9pGnTpunuu+9WcXGxlixZohUrVjR3SwAAoA1q9is58+bNUygU0rXXXquePXt62+LFi72a5557Tj/72c80atQoDRs2TIFAQL///e+9+Q4dOmj58uXq0KGDsrKydPvtt+vOO+/UY4895tX06dNHK1asUGFhoS6++GI988wzeu2115STk9PcLQEAgDao2a/kmNkpaxISEjR37lzNnTv3e2t69+6tlStXnvQ41157rTZv3nzaawQAAO7ju6sAAICTCDkAAMBJhBwAzWbQwkHRXgIAeAg5AADASYQcAADgJEIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHAAA4CRCDoATnD99RbPUAEA0EXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQdAi5g1axY3JwOIKkIOAABwEiEHAAA4iZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACcRcgAAgJMIOQAAwEmEHKCdOX/6imgvAQD+TxByAACAkwg5AADASYQcAADgJEIOAABwEiEHgHb07RftJQBAsyPkAIgqnvYC0FIIOQAAwEmEHAAA4CRCDgAAcBIhBwAAOImQA+D/1KCFg6K9BADtBCEHQKOeueVn0V4CAJwVQg6AqJk1a1a0lwDAYYQcAK0ev0sHwJkg5AAAACcRcgAAgJPafMiZO3euzj//fCUkJCgzM1MbNmyI9pKA6JmVFO0VAECr0aZDzuLFizVlyhQ9+uij+uSTT3TxxRcrJydH1dXV0V4aAACIsjYdcp599lmNHz9eY8eOVf/+/fXKK6+oU6dOev3116O9NAAAEGVx0V7AmTp8+LDKyso0Y8YMbyw2NlbZ2dkqKSlpdJ/a2lrV1tZ6r0OhkCQpHA637GKB5jD7PGnG305eU2vS///zvOfRj3Tur69U5WVDlFG2ySupr/3uhD/z++vqIsbqa7/ToSNHIsb219XpwIF6r7axmrqD/3uc2tpa1dd+pyduztGvFiw9ac3x67nirSv08W0f/6Pt2bNVXzuYv6cAPA3/PTCzkxdaG7Vnzx6TZB999FHE+NSpU+3yyy9vdJ9HH33UJLGxsbGxsbE5sO3evfukWaHNXsk5EzNmzNCUKVO81/X19dq7d6969OihmJiYZjtPOBxWenq6du/eLb/f32zHbe3aY9/tsWepffbdHnuW2mff7bFnqW31bWbat2+f0tLSTlrXZkPOOeecow4dOqiqqipivKqqSoFAoNF9fD6ffD5fxFhycnJLLVF+v7/V/0FpCe2x7/bYs9Q++26PPUvts+/22LPUdvpOSko6ZU2bvfE4Pj5el112mYqKiryx+vp6FRUVKSsrK4orAwAArUGbvZIjSVOmTNGYMWM0ZMgQXX755frtb3+rAwcOaOzYsdFeGgAAiLI2HXJuueUW/fd//7dmzpypYDCoSy65RKtXr1ZqampU1+Xz+fToo4+e8KMx17XHvttjz1L77Ls99iy1z77bY8+Sm33HmJ3q+SsAAIC2p83ekwMAAHAyhBwAAOAkQg4AAHASIQcAADiJkNOIvXv3Ki8vT36/X8nJyRo3bpz2799/0n0OHTqk/Px89ejRQ126dNGoUaNO+EWFu3btUm5urjp16qSUlBRNnTpVR48ejaj58MMPNXjwYPl8Pl144YVasGBBxPy8efP04x//2PtlTVlZWVq1apXTPc+ePVs/+clP1LVrV6WkpGjkyJGqrKw8655be9/r1q3TjTfeqLS0NMXExOjdd9894z7nzp2r888/XwkJCcrMzNSGDRtOWr906VL17dtXCQkJGjRokFauXBkxb2aaOXOmevbsqcTERGVnZ+uLL76IqGnKe7t161ZdffXVSkhIUHp6ugoKCs64x+O1xp4PHTqku+66S4MGDVJcXJxGjhzZbP02aI19f/jhh7rpppvUs2dPde7cWZdcconefPNNp3uurKzUddddp9TUVCUkJOiCCy7Qww8/rCNHjjjd97G+/PJLde3atUV/6e4pnfWXSDno+uuvt4svvtg+/vhj+6//+i+78MILbfTo0SfdZ+LEiZaenm5FRUW2adMmu+KKK+zKK6/05o8ePWoDBw607Oxs27x5s61cudLOOeccmzFjhlfzl7/8xTp16mRTpkyxiooKe/HFF61Dhw62evVqr2bZsmW2YsUK+/zzz62ystL+9V//1Tp27Gjbt293tuecnBx74403bPv27bZlyxb76U9/ar169bL9+/efVc+tve+VK1fav/3bv9nvf/97k2TvvPPOGfW4aNEii4+Pt9dff93Ky8tt/PjxlpycbFVVVY3Wr1+/3jp06GAFBQVWUVFhDz/8sHXs2NG2bdvm1Tz11FOWlJRk7777rn366af2T//0T9anTx87ePCgV3Oq9zYUCllqaqrl5eXZ9u3b7e2337bExER79dVXz6jPttDz/v37beLEiTZ//nzLycmxm2666ax7bQt9P/HEE/bwww/b+vXr7csvv7Tf/va3Fhsba++//76zPf/5z3+2119/3bZs2WJfffWVvffee5aSkhLx3wEX+25w+PBhGzJkiN1www2WlJTULD2fCULOcSoqKkySbdy40RtbtWqVxcTE2J49exrdp6amxjp27GhLly71xnbs2GGSrKSkxMz+8YEVGxtrwWDQq5k3b575/X6rra01M7Np06bZgAEDIo59yy23WE5OzknX3K1bN3vttddOr9FjtLWeq6urTZKtXbv29Js9Rlvq+2xCzuWXX275+fne67q6OktLS7PZs2c3Wv+LX/zCcnNzI8YyMzPtnnvuMTOz+vp6CwQCNmfOHG++pqbGfD6fvf3222bWtPf25Zdftm7dunnviZnZQw89ZBkZGWfU57Faa8/HGjNmTLOHnLbQd4Of/vSnNnbs2NNv8jhtqecHHnjAhg4devpNNqK19z1t2jS7/fbb7Y033ohqyOHHVccpKSlRcnKyhgwZ4o1lZ2crNjZWpaWlje5TVlamI0eOKDs72xvr27evevXqpZKSEu+4gwYNivhFhTk5OQqHwyovL/dqjj1GQ03DMY5XV1enRYsW6cCBA2f1VRZtqWdJCoVCkqTu3bufZqeR2lrfZ+Lw4cMqKyuLOFdsbKyys7O/91ynWtvOnTsVDAYjapKSkpSZmRnxHpzqvS0pKdGwYcMUHx8fcZ7Kykp9++23Tvbcktpa36FQ6Kz/Drelnr/88kutXr1a11xzzZk1e4zW3ndxcbGWLl2quXPnnnWvZ4uQc5xgMKiUlJSIsbi4OHXv3l3BYPB794mPjz/h546pqanePsFg8ITfxNzw+lQ14XBYBw8e9Ma2bdumLl26yOfzaeLEiXrnnXfUv3//02/2mPW39p4b1NfXa/Lkybrqqqs0cODApjf5PT20lb7P1P/8z/+orq6u0XOdrMeT1Tf881Q1p3pvm/I+nYnW3HNLakt9L1myRBs3bjzrr+BpCz1feeWVSkhI0EUXXaSrr75ajz322Gl2eaLW3Pc333yju+66SwsWLGgVX/LZbkLO9OnTFRMTc9Lts88+i/YymyQjI0NbtmxRaWmp7r33Xo0ZM0YVFRUn1LnUc4P8/Hxt375dixYt+t4aF/sGXLFmzRqNHTtWv/vd7zRgwIBoL6fFLV68WJ988oneeustrVixQk8//XS0l9Sixo8fr9tuu03Dhg2L9lIktfHvrjodDz74oO66666T1lxwwQUKBAKqrq6OGD969Kj27t2rQCDQ6H6BQECHDx9WTU1NxP/Dr6qq8vYJBAIn3Pne8ETOsTXHP6VTVVUlv9+vxMREbyw+Pl4XXnihJOmyyy7Txo0b9fzzz+vVV191tmdJmjRpkpYvX65169bpvPPO+96eXOv7bJxzzjnq0KFDo+c6WY8nq2/4Z1VVlXr27BlRc8kll3g1p3pvv+88x57jTLTmnltSW+h77dq1uvHGG/Xcc8/pzjvvPP0mj9MWek5PT5ck9e/fX3V1dZowYYIefPBBdejQ4TS7/V+tue/i4mItW7bMC3Nmpvr6esXFxWn+/Pm6++67z7DrMxS1u4FaqYYbqzZt2uSNffDBB026GfU///M/vbHPPvus0ZtRj73z/dVXXzW/32+HDh0ys3/cqDVw4MCIY48ePfqUNx5fd911NmbMmNPq81itvef6+nrLz8+3tLQ0+/zzz8+4z+O19r6PpbO88XjSpEne67q6Ojv33HNPeoPiz372s4ixrKysE25QfPrpp735UCjU6A2KJ3tvG248Pnz4sFczY8aMZrvxuDX2fKyWuvG4tfa9Zs0a69y5s7300ktn3+gxWnPPx1u4cKHFxcVF/Jk/U62174qKCtu2bZu3Pf7449a1a1fbtm2b7d2796z7Pl2EnEZcf/31dumll1ppaan96U9/sosuuijiEbm//e1vlpGRYaWlpd7YxIkTrVevXlZcXGybNm2yrKwsy8rK8uYbHiseMWKEbdmyxVavXm0/+MEPGn2seOrUqbZjxw6bO3fuCY8VT58+3dauXWs7d+60rVu32vTp0y0mJsb+8Ic/ONvzvffea0lJSfbhhx/a3//+d2/77rvvzqrn1t73vn37bPPmzbZ582aTZM8++6xt3rzZ/vrXv55Wj4sWLTKfz2cLFiywiooKmzBhgiUnJ3tPf91xxx02ffp0r379+vUWFxdnTz/9tO3YscMeffTRRh81TU5Otvfee8+2bt1qN910U6OPmp7sva2pqbHU1FS74447bPv27bZo0SLr1KlTsz1C3hp7NjMrLy+3zZs324033mjXXnut9++4ObTWvouLi61Tp042Y8aMiL/D33zzjbM9/8d//IctXrzYKioq7M9//rMtXrzY0tLSLC8v76x7bs19Hy/aT1cRchrxzTff2OjRo61Lly7m9/tt7Nixtm/fPm9+586dJsnWrFnjjR08eNDuu+8+69atm3Xq1Ml+/vOf29///veI43711Vd2ww03WGJiop1zzjn24IMP2pEjRyJq1qxZY5dcconFx8fbBRdcYG+88UbE/N133229e/e2+Ph4+8EPfmDDhw8/64DT2nuW1Oh2fJ1rfa9Zs6bRvs/kqt2LL75ovXr1svj4eLv88svt448/9uauueaaE465ZMkS+9GPfmTx8fE2YMAAW7FiRcR8fX29PfLII5aammo+n8+GDx9ulZWVETWnem/NzD799FMbOnSo+Xw+O/fcc+2pp5467d7aWs+9e/du9N+ry32PGTOm0Z6vueYaZ3tetGiRDR482Lp06WKdO3e2/v3725NPPhkRGFzs+3jRDjkxZmYt+vMwAACAKGg3T1cBAID2hZADAACcRMgBAABOIuQAAAAnEXIAAICTCDkAAMBJhBwAAOAkQg4AAHASIQcAADiJkAMAAJxEyAEAAE4i5AAAACf9PypUYxnLPHMyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = plt.hist(unscaled_pt.numpy() - reco_particles[:,:,0].numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d506e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([103847.,  20953.,  19518.,  12656.,  13181.,   5135.,   3276.,\n",
       "          1425.]),\n",
       " array([   0.,   25.,   50.,   75.,  100.,  150.,  200.,  300., 1000.]),\n",
       " <BarContainer object of 8 artists>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(unscaled_pt.numpy().flatten(), bins=[0,25,50,75,100,150,200,300,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82eaf33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([247.5000, 130.2500,  89.1250,  86.8750,  55.2813,  33.0312,  33.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,  44.1741,  93.1483])\n"
     ]
    }
   ],
   "source": [
    "print(unscaled_pt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f7e7ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "mask_pt_greater_0 = unscaled_pt[:,:10] > 5 \n",
    "mask_pt_lower_50 = unscaled_pt[:,:10] < 30\n",
    "\n",
    "mask_pt_0_50 = torch.logical_and(mask_pt_grater_0, mask_pt_lower_50)\n",
    "print(mask_pt_0_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89a103a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1403.0004)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(unscaled_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92944b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "print(flow_pr[mask_pt_0_50].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9aa0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-36.4432,  -8.4614, -11.9981,  ...,   0.5705,   0.5705,   0.5705],\n",
      "        [-13.8718, -23.8085,  -3.3919,  ...,   0.5705,   0.5705,   0.5705],\n",
      "        [-40.0943, -20.8213,  -5.8146,  ...,   0.5705,   0.5705,   0.5705],\n",
      "        ...,\n",
      "        [ -6.1844, -15.3687, -24.3188,  ...,   0.5705,   0.5705,   0.5705],\n",
      "        [-34.4876, -13.8191,  -6.4025,  ...,   0.5705,   0.5705,   0.5705],\n",
      "        [-17.1896,  -0.2763, -30.9214,  ...,   0.5705,   0.5705,   0.5705]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(flow_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75854a3",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5638c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Loading datasets\n",
      "Loading partons in LAB\n",
      "PartonLevel LAB\n",
      "Reading parton_level Files\n",
      "Load logScaled_data_higgs_t_tbar_ISR\n",
      "Parton: Move tensors to device (None) memory\n",
      "Loading reco in LAB\n",
      "RecoLevel LAB\n",
      "Reading reco_level Files\n",
      "Load scaledLogRecoParticles\n",
      "Reco: Move tensors to device (None) memory\n",
      "Loaded datasets:  ['partons_lab', 'reco_lab']\n"
     ]
    }
   ],
   "source": [
    "world_size = None\n",
    "\n",
    "data = dataset_all.DatasetCombined(root, dtype=torch.float64, datasets=[\"partons_lab\", \"reco_lab\"],\n",
    "                                    reco_list_lab=['scaledLogRecoParticles', 'mask_lepton',\n",
    "                                                'mask_jets','mask_met',\n",
    "                                                'mask_boost', 'data_boost'],\n",
    "                                    parton_list_lab=['logScaled_data_higgs_t_tbar_ISR'])\n",
    "\n",
    "trainingLoader = DataLoader(\n",
    "        data,\n",
    "        batch_size= 32,\n",
    "        shuffle=False if world_size is not None else True,\n",
    "        sampler=DistributedSampler(train_dataset) if world_size is not None else None,\n",
    "\n",
    "    )\n",
    "\n",
    "optimizer = optim.Adam(list(tr_flow.parameters()) , lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30234b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_conf = ''\n",
    "model_weights = ''\n",
    "with open(path_to_conf) as f:\n",
    "        config = OmegaConf.load(path_to_conf)\n",
    "\n",
    "if True:    \n",
    "    \n",
    "    log_mean_parton = data.partons_lab.mean_log_data_higgs_t_tbar_ISR\n",
    "    log_std_parton = data.partons_lab.std_log_data_higgs_t_tbar_ISR\n",
    "    log_mean_boost = data.partons_lab.mean_log_data_boost\n",
    "    log_std_boost = data.partons_lab.std_log_data_boost\n",
    "    mean_ps = data.partons_CM.mean_phasespace_intermediateParticles_onShell_logit\n",
    "    scale_ps = data.partons_CM.std_phasespace_intermediateParticles_onShell_logit\n",
    "\n",
    "    # Initialize model\n",
    "    model = UnfoldingFlow(\n",
    "                    pretrained_model=None,\n",
    "                    load_conditioning_model=False,\n",
    "                    scaling_partons_lab = [log_mean_parton, log_std_parton],\n",
    "                    scaling_boost_lab = [log_mean_boost, log_std_boost],\n",
    "                    scaling_partons_CM_ps = [mean_ps, scale_ps],\n",
    "\n",
    "                    no_jets=config.input_shape.number_jets,\n",
    "                    no_lept=config.input_shape.number_lept,\n",
    "                    input_features=config.input_shape.input_features,\n",
    "                    cond_hiddenFeatures=config.conditioning_transformer.hidden_features,\n",
    "                    cond_dimFeedForward=config.conditioning_transformer.dim_feedforward_transformer,\n",
    "                    cond_outFeatures=config.conditioning_transformer.out_features,\n",
    "                    cond_nheadEncoder=config.conditioning_transformer.nhead_encoder,\n",
    "                    cond_NoLayersEncoder=config.conditioning_transformer.no_layers_encoder,\n",
    "                    cond_nheadDecoder=config.conditioning_transformer.nhead_decoder,\n",
    "                    cond_NoLayersDecoder=config.conditioning_transformer.no_layers_decoder,\n",
    "                    cond_NoDecoders=config.conditioning_transformer.no_decoders,\n",
    "                    cond_aggregate=config.conditioning_transformer.aggregate,\n",
    "                    cond_use_latent=config.conditioning_transformer.use_latent,\n",
    "                    cond_out_features_latent=config.conditioning_transformer.out_features_latent,\n",
    "                    cond_no_layers_decoder_latent=config.conditioning_transformer.no_layers_decoder_latent,   \n",
    "        \n",
    "                    flow_nfeatures=config.unfolding_flow.nfeatures,\n",
    "                    flow_ncond=config.unfolding_flow.ncond, \n",
    "                    flow_ntransforms=config.unfolding_flow.ntransforms,\n",
    "                    flow_hiddenMLP_NoLayers=config.unfolding_flow.hiddenMLP_NoLayers, \n",
    "                    flow_hiddenMLP_LayerDim=config.unfolding_flow.hiddenMLP_LayerDim,\n",
    "                    flow_bins=config.unfolding_flow.bins,\n",
    "                    flow_autoregressive=config.unfolding_flow.autoregressive,\n",
    "                    flow_base=config.unfolding_flow.base,\n",
    "                    flow_base_first_arg=config.unfolding_flow.base_first_arg,\n",
    "                    flow_base_second_arg=config.unfolding_flow.base_second_arg,\n",
    "                    flow_bound=config.unfolding_flow.bound,\n",
    "                    randPerm=config.unfolding_flow.randPerm,\n",
    "                    device=torch.device(\"cpu\"),\n",
    "                    dtype=torch.float64,\n",
    "                    eps=config.training_params.eps)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_weights, map_location=torch.device(\"cpu\"))[\"model_state_dict\"])\n",
    "\n",
    "    # Setting up DDP\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e97374f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training loop\n",
      "-83.9643303985508\n",
      "-102.6012157631452\n",
      "-101.42285277269741\n",
      "-108.31054313249706\n",
      "-103.96656619890943\n",
      "-103.59378531821196\n",
      "-96.37625334310928\n",
      "-106.23000139640905\n",
      "-91.70372009474264\n",
      "-92.2443125884378\n",
      "-87.99381986408463\n",
      "-100.92204492388557\n",
      "-92.25718547468938\n",
      "-97.2608279975494\n",
      "-101.89203255683735\n",
      "-93.43479184751993\n",
      "-102.00784309151845\n",
      "-101.07386803979583\n",
      "-101.2330457123895\n",
      "-83.99931513651171\n",
      "-95.1283589498004\n",
      "-82.1406976379478\n",
      "-89.02669805160156\n",
      "-107.4434084363919\n",
      "-100.97298676694763\n",
      "-92.12522791126152\n",
      "-90.62383540629529\n",
      "-102.51235729300683\n",
      "-85.55894213153087\n",
      "-101.31168416104993\n",
      "-97.26396071372214\n",
      "-98.66523504040597\n",
      "-100.47264554641583\n",
      "-104.21245099296019\n",
      "-89.22807566120694\n",
      "-92.64143616037029\n",
      "-87.2705644119667\n",
      "-112.57627197358906\n",
      "-110.07537037154756\n",
      "-86.70101794516208\n",
      "-102.52339218172689\n",
      "-77.83094293868473\n",
      "-94.19359068204841\n",
      "-89.04236757066963\n",
      "-99.12886901950682\n",
      "-95.23693030500418\n",
      "-85.73071781711835\n",
      "-91.37078379789206\n",
      "-90.79807893334043\n",
      "-96.24954718880326\n",
      "-95.84560587455135\n",
      "-82.11984920193737\n",
      "-89.69831819639717\n",
      "-91.01745967231278\n",
      "-96.72843564428509\n",
      "-94.86805334725692\n",
      "-99.34374404431188\n",
      "-99.4997708023541\n",
      "-107.32354593389252\n",
      "-91.89388464297126\n",
      "-98.23768579625676\n",
      "-98.31407346371665\n",
      "-85.74057587674298\n",
      "-92.69876339468154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     scaledLogRecoParticles \u001b[38;5;241m=\u001b[39m scaledLogRecoParticles[:,:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 56\u001b[0m avg_flowPr, batch_flow_pr, flow_pr \u001b[38;5;241m=\u001b[39m \u001b[43mtr_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaledLogRecoParticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogScaled_data_higgs_t_tbar_ISR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdata_boost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_recoParticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_boost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m avg_flowPr\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/work/adpetre/miniconda3/envs/dizertatie/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[54], line 82\u001b[0m, in \u001b[0;36mTransferFlow.forward\u001b[0;34m(self, scaling_reco_lab, scaling_partons_lab, scaling_RegressedBoost_lab, mask_reco, mask_boost, batch_size, no_objects, flow_eval, Nsamples)\u001b[0m\n\u001b[1;32m     78\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_model\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(scaledLogReco_afterLin\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m output_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin,\n\u001b[1;32m     80\u001b[0m                                   tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n\u001b[0;32m---> 82\u001b[0m flow_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_decoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mno_objects\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaling_reco_lab\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mno_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#tgt_mask = self.transformer_model.generate_square_subsequent_mask(scaledLogReco_withBoost_afterLin.size(1))\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#output_decoder = self.transformer_model(scaledLogParton_afterLin, scaledLogReco_withBoost_afterLin,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#                                  tgt_mask=tgt_mask)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#flow_prob = self.flow(output_decoder[:,:no_objects]).log_prob(scaling_reco_lab_andBoost[:,:no_objects])\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# i will want to add the boost in the target too\u001b[39;00m\n\u001b[1;32m     93\u001b[0m flow_prob_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(flow_prob\u001b[38;5;241m*\u001b[39mmask_reco[:,:no_objects], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# take avg of masked objects\u001b[39;00m\n",
      "File \u001b[0;32m/work/adpetre/zuko/zuko/distributions.py:110\u001b[0m, in \u001b[0;36mNormalizingFlow.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 110\u001b[0m     z, ladj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_and_ladj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     ladj \u001b[38;5;241m=\u001b[39m _sum_rightmost(ladj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mlog_prob(z) \u001b[38;5;241m+\u001b[39m ladj\n",
      "File \u001b[0;32m/work/adpetre/zuko/zuko/transforms.py:125\u001b[0m, in \u001b[0;36mComposedTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 125\u001b[0m     x, ladj \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_and_ladj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     acc \u001b[38;5;241m=\u001b[39m acc \u001b[38;5;241m+\u001b[39m _sum_rightmost(ladj, event_dim \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mevent_dim)\n\u001b[1;32m    127\u001b[0m     event_dim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcodomain\u001b[38;5;241m.\u001b[39mevent_dim \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mevent_dim\n",
      "File \u001b[0;32m/work/adpetre/zuko/zuko/transforms.py:735\u001b[0m, in \u001b[0;36mAutoregressiveTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_and_ladj\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m--> 735\u001b[0m     y, ladj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_and_ladj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, ladj\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/work/adpetre/zuko/zuko/transforms.py:460\u001b[0m, in \u001b[0;36mMonotonicRQSTransform.call_and_ladj\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    451\u001b[0m z \u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m-\u001b[39m x0) \u001b[38;5;241m/\u001b[39m (x1 \u001b[38;5;241m-\u001b[39m x0)\n\u001b[1;32m    453\u001b[0m y \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m (y1 \u001b[38;5;241m-\u001b[39m y0) \u001b[38;5;241m*\u001b[39m (s \u001b[38;5;241m*\u001b[39m z\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m d0 \u001b[38;5;241m*\u001b[39m z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z)) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m    454\u001b[0m     s \u001b[38;5;241m+\u001b[39m (d0 \u001b[38;5;241m+\u001b[39m d1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m s) \u001b[38;5;241m*\u001b[39m z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z)\n\u001b[1;32m    455\u001b[0m )\n\u001b[1;32m    457\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m     s\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m s \u001b[38;5;241m*\u001b[39m z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z) \u001b[38;5;241m+\u001b[39m d0 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m z) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m d1 \u001b[38;5;241m*\u001b[39m z\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;241m/\u001b[39m \u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43md0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    461\u001b[0m )\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(mask, y, x), mask \u001b[38;5;241m*\u001b[39m jacobian\u001b[38;5;241m.\u001b[39mlog()\n",
      "File \u001b[0;32m/work/adpetre/miniconda3/envs/dizertatie/lib/python3.11/site-packages/torch/_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_flow = TransferFlow(no_recoVars=7, no_partonVars=3,\n",
    "                 out_features_linearDNN=64,\n",
    "                 \n",
    "                 transformer_nhead=4,\n",
    "                 transformer_num_encoder_layers=4,\n",
    "                 transformer_num_decoder_layers=4,\n",
    "                 transformer_dim_feedforward=64,\n",
    "                 transformer_activation=nn.GELU(),\n",
    "                 \n",
    "                 flow_nfeatures=3,\n",
    "                 flow_ncond=34, \n",
    "                 flow_ntransforms=5,\n",
    "                 flow_hiddenMLP_NoLayers=16,\n",
    "                 flow_hiddenMLP_LayerDim=128,\n",
    "                 flow_bins=16,\n",
    "                 flow_autoregressive=True,\n",
    "                 flow_base='DiagNormal',\n",
    "                 flow_base_first_arg=0,\n",
    "                 flow_base_second_arg=0.3,\n",
    "                 flow_bound=1.,\n",
    "                 randPerm=False,\n",
    "                 \n",
    "                 device=torch.device('cpu'),\n",
    "                 dtype=torch.float32,\n",
    "                 eps=1e-4)\n",
    "\n",
    "ii = 0\n",
    "for e in range(10):\n",
    "          \n",
    "        N_train = 0\n",
    "        N_valid = 0\n",
    "            \n",
    "        sum_loss = 0.\n",
    "    \n",
    "        # training loop    \n",
    "        print(\"Before training loop\")\n",
    "        tr_flow.train()\n",
    "\n",
    "        for i, data_batch in enumerate(trainingLoader):\n",
    "            N_train += 1\n",
    "            ii+=1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            (logScaled_data_higgs_t_tbar_ISR,\n",
    "             scaledLogRecoParticles, mask_lepton,\n",
    "             mask_jets, mask_met,\n",
    "             mask_boost, data_boost) = data_batch\n",
    "            \n",
    "            mask_recoParticles = torch.cat((mask_jets, mask_lepton, mask_met), dim=1)\n",
    "\n",
    "            # remove prov\n",
    "            if True:\n",
    "                scaledLogRecoParticles = scaledLogRecoParticles[:,:,:-1]\n",
    "\n",
    "            avg_flowPr, batch_flow_pr, flow_pr = tr_flow(scaledLogRecoParticles, logScaled_data_higgs_t_tbar_ISR,\n",
    "                                                         data_boost, mask_recoParticles, mask_boost,\n",
    "                                                         32, 10)\n",
    "                    \n",
    "\n",
    "            avg_flowPr.backward()\n",
    "            optimizer.step()\n",
    "            print(avg_flowPr.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6388c7f5",
   "metadata": {},
   "source": [
    "# version from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09766290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "from memflow.unfolding_network.conditional_transformer import ConditioningTransformerLayer\n",
    "import zuko\n",
    "from zuko.flows import TransformModule, SimpleAffineTransform\n",
    "from zuko.distributions import BoxUniform\n",
    "from zuko.distributions import DiagNormal\n",
    "from memflow.unfolding_flow.utils import Compute_ParticlesTensor as particle_tools\n",
    "import memflow.phasespace.utils as ps_utils\n",
    "\n",
    "class TransferFlow_Paper(nn.Module):\n",
    "    def __init__(self,\n",
    "                 no_recoVars, no_partonVars,\n",
    "                 \n",
    "                 transformer_input_features=64,\n",
    "                 transformer_nhead=8,\n",
    "                 transformer_num_encoder_layers=4,\n",
    "                 transformer_num_decoder_layers=4,\n",
    "                 transformer_dim_feedforward=128,\n",
    "                 transformer_activation=nn.GELU(),\n",
    "                 \n",
    "                 flow_nfeatures=12,\n",
    "                 flow_ntransforms=5,\n",
    "                 flow_hiddenMLP_NoLayers=4,\n",
    "                 flow_hiddenMLP_LayerDim=128,\n",
    "                 flow_bins=16,\n",
    "                 flow_autoregressive=True,\n",
    "                 flow_base=BoxUniform,\n",
    "                 flow_base_first_arg=-1,\n",
    "                 flow_base_second_arg=1,\n",
    "                 flow_bound=1.,\n",
    "                 randPerm=False,\n",
    "                 no_max_objects=10,\n",
    "                 \n",
    "                 device=torch.device('cpu'),\n",
    "                 dtype=torch.float32,\n",
    "                 eps=1e-4):\n",
    "\n",
    "        super(TransferFlow_Paper, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.eps = eps # used for small values like the mass of the gluon for numerical reasons\n",
    "        \n",
    "        self.linearDNN_reco = nn.Linear(in_features=no_recoVars, out_features=transformer_input_features)\n",
    "        self.linearDNN_parton = nn.Linear(in_features=no_partonVars, out_features=transformer_input_features)\n",
    "        self.linearDNN_boost = nn.Linear(in_features=4, out_features=transformer_input_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.no_max_objects = no_max_objects\n",
    "        \n",
    "        self.transformer_model = nn.Transformer(d_model=transformer_input_features,\n",
    "                                                nhead=transformer_nhead,\n",
    "                                                num_encoder_layers=transformer_num_encoder_layers,\n",
    "                                                num_decoder_layers=transformer_num_decoder_layers,\n",
    "                                                dim_feedforward=transformer_dim_feedforward,\n",
    "                                                activation=transformer_activation,\n",
    "                                                batch_first=True)\n",
    "        \n",
    "        self.flow_pt = zuko.flows.NSF(features=flow_nfeatures,\n",
    "                              context=transformer_input_features,\n",
    "                              transforms=flow_ntransforms, \n",
    "                              bins=flow_bins, \n",
    "                              hidden_features=[flow_hiddenMLP_LayerDim]*flow_hiddenMLP_NoLayers, \n",
    "                              randperm=randPerm,\n",
    "                              base=eval(flow_base),\n",
    "                              base_args=[torch.ones(flow_nfeatures)*flow_base_first_arg, torch.ones(flow_nfeatures)*flow_base_second_arg],\n",
    "                                   univariate_kwargs={\"bound\": flow_bound }, # Keeping the flow in the [-B,B] box.\n",
    "                              passes= 2 if not flow_autoregressive else flow_nfeatures)\n",
    "\n",
    "        self.flow_eta = zuko.flows.NSF(features=flow_nfeatures,\n",
    "                              context=transformer_input_features + 1, # additional condition pt\n",
    "                              transforms=flow_ntransforms, \n",
    "                              bins=flow_bins, \n",
    "                              hidden_features=[flow_hiddenMLP_LayerDim]*flow_hiddenMLP_NoLayers, \n",
    "                              randperm=randPerm,\n",
    "                              base=eval(flow_base),\n",
    "                              base_args=[torch.ones(flow_nfeatures)*flow_base_first_arg, torch.ones(flow_nfeatures)*flow_base_second_arg],\n",
    "                                   univariate_kwargs={\"bound\": flow_bound }, # Keeping the flow in the [-B,B] box.\n",
    "                              passes= 2 if not flow_autoregressive else flow_nfeatures)\n",
    "\n",
    "        self.flow_phi = zuko.flows.NSF(features=flow_nfeatures,\n",
    "                              context=transformer_input_features + 2, # additional condition pt/eta\n",
    "                              transforms=flow_ntransforms, \n",
    "                              bins=flow_bins, \n",
    "                              hidden_features=[flow_hiddenMLP_LayerDim]*flow_hiddenMLP_NoLayers, \n",
    "                              randperm=randPerm,\n",
    "                              base=eval(flow_base),\n",
    "                              base_args=[torch.ones(flow_nfeatures)*flow_base_first_arg, torch.ones(flow_nfeatures)*flow_base_second_arg],\n",
    "                                   univariate_kwargs={\"bound\": flow_bound }, # Keeping the flow in the [-B,B] box.\n",
    "                              passes= 2 if not flow_autoregressive else flow_nfeatures)\n",
    "        \n",
    "\n",
    "    def disable_conditioner_regression_training(self):\n",
    "        ''' Disable the conditioner regression training, but keep the\n",
    "        latent space training'''\n",
    "        self.cond_transformer.disable_regression_training()\n",
    "\n",
    "    def enable_regression_training(self):\n",
    "        self.cond_transformer.enable_regression_training()\n",
    "        \n",
    "    def forward(self,  scaling_reco_lab, scaling_partons_lab, scaling_RegressedBoost_lab,\n",
    "                mask_reco, mask_boost, flow_eval=\"normalizing\", Nsamples=0):\n",
    "        \n",
    "        scaledLogReco_afterLin = self.gelu(self.linearDNN_reco(scaling_reco_lab) * mask_reco[..., None])\n",
    "        scaledLogParton_afterLin = self.gelu(self.linearDNN_parton(scaling_partons_lab))\n",
    "\n",
    "        tgt_mask = self.transformer_model.generate_square_subsequent_mask(scaledLogReco_afterLin.size(1), device=self.device)\n",
    "        # mask to keep the decoder autoregressive\n",
    "        \n",
    "        output_decoder = self.transformer_model(scaledLogParton_afterLin, scaledLogReco_afterLin,\n",
    "                                                tgt_mask=tgt_mask)\n",
    "        \n",
    "        conditioning_pt = output_decoder[:,:self.no_max_objects]\n",
    "        scaled_reco_lab_pt = scaling_reco_lab[:,:self.no_max_objects,0].unsqueeze(dim=2)\n",
    "        flow_prob_pt = self.flow_pt(conditioning_pt).log_prob(scaled_reco_lab_pt)\n",
    "        flow_prob_pt_batch = torch.sum(flow_prob_pt*mask_reco[:,:self.no_max_objects], dim=1) # take avg of masked objects\n",
    "        avg_flow_prob_pt = flow_prob_pt_batch.mean()\n",
    "        print(flow_prob_pt.shape)\n",
    "        \n",
    "        conditioning_eta = torch.cat((output_decoder[:,:self.no_max_objects], scaled_reco_lab_pt), dim=2) # add pt in conditioning\n",
    "        scaled_reco_lab_eta = scaling_reco_lab[:,:self.no_max_objects,1].unsqueeze(dim=2)\n",
    "        flow_prob_eta = self.flow_eta(conditioning_eta).log_prob(scaled_reco_lab_eta)\n",
    "        flow_prob_eta_batch = torch.sum(flow_prob_eta*mask_reco[:,:self.no_max_objects], dim=1) # take avg of masked objects\n",
    "        avg_flow_prob_eta = flow_prob_eta_batch.mean()\n",
    "        print(flow_prob_eta.shape)\n",
    "        \n",
    "        conditioning_phi = torch.cat((output_decoder[:,:self.no_max_objects], scaled_reco_lab_pt, scaled_reco_lab_eta), dim=2)\n",
    "        scaled_reco_lab_phi = scaling_reco_lab[:,:self.no_max_objects,2].unsqueeze(dim=2)\n",
    "        flow_prob_phi = self.flow_phi(conditioning_phi).log_prob(scaled_reco_lab_phi)\n",
    "        flow_prob_phi_batch = torch.sum(flow_prob_phi*mask_reco[:,:self.no_max_objects], dim=1) # take avg of masked objects\n",
    "        avg_flow_prob_phi = flow_prob_phi_batch.mean()\n",
    "        print(flow_prob_phi.shape)\n",
    "                                \n",
    "        return avg_flow_prob_pt, flow_prob_pt_batch, flow_prob_pt, \\\n",
    "                avg_flow_prob_eta, flow_prob_eta_batch, flow_prob_eta, \\\n",
    "                avg_flow_prob_phi, flow_prob_phi_batch, flow_prob_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2b2e86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move to cuda\n"
     ]
    }
   ],
   "source": [
    "tr_flow = None\n",
    "#tr_flow.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del tr_flow\n",
    "\n",
    "tr_flow = TransferFlow_Paper(no_recoVars=7, no_partonVars=3,\n",
    "                 transformer_input_features=32,\n",
    "                 \n",
    "                 transformer_nhead=2,\n",
    "                 transformer_num_encoder_layers=1,\n",
    "                 transformer_num_decoder_layers=1,\n",
    "                 transformer_dim_feedforward=32,\n",
    "                 transformer_activation=nn.GELU(),\n",
    "                 \n",
    "                 flow_nfeatures=1,\n",
    "                 flow_ntransforms=5,\n",
    "                 flow_hiddenMLP_NoLayers=16,\n",
    "                 flow_hiddenMLP_LayerDim=128,\n",
    "                 flow_bins=16,\n",
    "                 flow_autoregressive=True,\n",
    "                 flow_base='DiagNormal',\n",
    "                 flow_base_first_arg=0,\n",
    "                 flow_base_second_arg=0.3,\n",
    "                 flow_bound=1.,\n",
    "                 randPerm=False,\n",
    "                 \n",
    "                 device=device,\n",
    "                 dtype=torch.float32,\n",
    "                 eps=1e-4)\n",
    "\n",
    "if device == torch.device('cuda'):\n",
    "    print('move to cuda')\n",
    "    tr_flow = tr_flow.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76bc0cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/micromamba/lib/python3.9/site-packages/zuko/transforms.py:425: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/BucketizationUtils.h:33.)\n",
      "  return torch.searchsorted(seq, value[..., None]).squeeze(dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "avg_flow_prob_pt, flow_prob_pt_batch, flow_prob_pt, \\\n",
    "avg_flow_prob_eta, flow_prob_eta_batch, flow_prob_eta, \\\n",
    "avg_flow_prob_phi, flow_prob_phi_batch, flow_prob_phi = tr_flow(scaledLogReco[:,:,:7], scaledLogParton, boostReco, maskedReco, maskBoost,\n",
    "                                             10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bcebb-fee7-49c7-b568-2beb7a40b447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
